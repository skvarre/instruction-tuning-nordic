{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting a subset of OpenHermes-2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/OpenHermes-2.5.jsonl', lines=True)\n",
    "\n",
    "print(df.head())\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"source\"].unique()\n",
    "\n",
    "# Make a new dataset with only the English articles and None\n",
    "df_en = df[df[\"language\"].isin([\"English\", None])]\n",
    "\n",
    "values = df_en[\"source\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total = 100_000\n",
    "\n",
    "new_values = pd.Series()\n",
    "\n",
    "for category, count in values.items():\n",
    "    # Normalize\n",
    "    new_values[category] = int(count / len(df_en) * num_total)\n",
    "\n",
    "new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en[\"source\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = pd.DataFrame()\n",
    "\n",
    "for category, count in new_values.items():\n",
    "    if pd.isna(category):\n",
    "        sampled_df = df_en[df_en[\"source\"].isna()].sample(n=count, random_state=42)\n",
    "    else:\n",
    "        sampled_df = df_en[df_en[\"source\"] == category].sample(n=count, random_state=42)\n",
    "    reduced_df = pd.concat([reduced_df, sampled_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the entire dataset\n",
    "reduced_df = reduced_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = reduced_df.iloc[0]['conversations'][0]\n",
    "convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reduced_df to a jsonl file\n",
    "reduced_df.to_json('../data/OpenHermes-2.5-100k.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = pd.read_json('../data/OpenHermes-2.5-100k.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLEMAP = {'human' : '<human>', 'gpt' : '<bot>', 'system' : 'content'}\n",
    "\n",
    "test = reduced_df.iloc[0]['conversations']\n",
    "new_json = \"test.jsonl\"\n",
    "\n",
    "with open(new_json, 'w') as f:\n",
    "    for i, row in reduced_df.iterrows():\n",
    "        turns = []\n",
    "        conversations = row['conversations']\n",
    "        for conv in conversations: \n",
    "            role = ROLEMAP[conv['from']]\n",
    "            text = conv['value']\n",
    "\n",
    "\n",
    "#for i in range(10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"OpenHermes-2.5-300k.jsonl\"\n",
    "output = \"test.jsonl\"\n",
    "\n",
    "with open(input, 'r') as f:\n",
    "    with open(output, 'w') as g:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            conv_dict = data['conversations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting .apkg anki-file and creating a dataset from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '../collection.anki21'\n",
    "\n",
    "con = sqlite3.connect(db_path)\n",
    "query = \"\"\"\n",
    "SELECT id, flds, tags FROM notes;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    print(row['flds'].split('\\x1f')[1]), print(row['flds'].split('\\x1f')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../hogskoleprovet-ord-8.5k.jsonl', 'w', encoding='utf-8') as file:\n",
    "    for _, row in df.iterrows():\n",
    "        data = {\n",
    "            'metadata': 'hogskoleprovet-ord',\n",
    "            'text': row['flds'].split('\\x1f')\n",
    "        }\n",
    "        data['text'][1] = f'{data['text'][0].capitalize()} betyder {data['text'][1]}' \n",
    "        data['text'][0] = f'Vad betyder {data['text'][0]}?'\n",
    "        json.dump(data, file)\n",
    "        file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../en-swe.jsonl', 'a', encoding='utf-8') as file:\n",
    "    for _, row in df.iterrows():\n",
    "        extract_rows = row['flds'].split('\\x1f')\n",
    "        if \"en\" in extract_rows[1] or \"ett\" in extract_rows[1] or \"att\" in extract_rows[1]:\n",
    "            continue \n",
    "        data = {\n",
    "            \"en\":extract_rows[3],\n",
    "            \"sv\":extract_rows[1]\n",
    "        }\n",
    "        json.dump(data, file)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data into the format I use for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    " \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['conversations'][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OpenHermes-28k-hsp-3k.jsonl', 'w', encoding='utf-8') as file:\n",
    "    for line in data:\n",
    "        new_dict = {\n",
    "            'text':[]\n",
    "        }\n",
    "        lst = []\n",
    "        question_string = \"\"\n",
    "        for convs in line['conversations'][:-1]:\n",
    "            question_string += f\"\\n{convs['value']}\" if question_string else convs['value']\n",
    "        question = {\"<human>\" : question_string}\n",
    "        lst.append(question)\n",
    "        answer = {\"<bot>\" : line['conversations'][-1]['value']}\n",
    "        lst.append(answer)\n",
    "        new_dict['text'] = lst\n",
    "        json.dump(new_dict, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/hogskoleprovet-ord-8.5k.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    " \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OpenHermes-28k-hsp-3k.jsonl', 'a', encoding='utf-8') as file:\n",
    "    for line in data:\n",
    "        new_dict = {\n",
    "            'text':[\n",
    "                {\"<human>\":line['text'][0]},\n",
    "                {\"<bot>\":line['text'][1]}\n",
    "            ]\n",
    "        }\n",
    "        json.dump(new_dict, file)\n",
    "        file.write(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data in 'OpenHermes-28k-hsp-8k.jsonl'\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open('OpenHermes-28k-hsp-8k.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "with open('OpenHermes-28k-hsp-8k-SHUFFLED.jsonl', 'w', encoding='utf-8') as file:\n",
    "    for line in data:\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the data in 'OpenHermes-28k-hsp-8k-SHUFFLED.jsonl' and delete every line that has the occurence of '<|endoftext|>'ArithmeticError\n",
    "import json\n",
    "\n",
    "with open('OpenHermes-28k-hsp-8k-SHUFFLED.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "new_data = []\n",
    "for line in data:\n",
    "    if line['text'][0]['<human>'].find('<|endoftext|>') == -1 and line['text'][1]['<bot>'].find('<|endoftext|>') == -1:\n",
    "        new_data.append(line)\n",
    "\n",
    "with open('OpenHermes-28k-hsp-8k-SHUFFLED.jsonl', 'w', encoding='utf-8') as file:\n",
    "    for line in new_data:\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting english rows from SlimOrca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = \"../data/SlimOrca-sv-CONTINUE-3.jsonl\"\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "data \n",
    "\n",
    "with open(path, 'w', encoding='utf-8') as file:\n",
    "    for line in data:\n",
    "        # Delete 0:th index from conversations value\n",
    "        for conv in line['conversations']:\n",
    "            # MAke it a string and not a list\n",
    "            conv['value'] = conv['value'][1]\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting english rows from DPO-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = \"../data/Orca-DPO-pairs-geq2k.jsonl\"\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../data/Orca-DPO-pairs-geq2k-extracted.jsonl\"\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    for line in data:\n",
    "        if line['system'] != \"\\\"\\\"\":\n",
    "            line['system'] = json.loads(line['system'])[1]\n",
    "        else:\n",
    "            line['system'] = \"\"\n",
    "        line['question'] = line['question'][1]\n",
    "        line['chosen'] = line['chosen'][1]\n",
    "        line['rejected'] = line['rejected'][1]\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by only 'conversations' entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all entries in the json except conversations\n",
    "import json\n",
    "\n",
    "path = \"../data/CamelAI-7k-sv-2.jsonl\"\n",
    "\n",
    "with open(path , 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "\n",
    "with open(path, 'w', encoding='utf-8') as file:\n",
    "    for line in data:\n",
    "        new_dict = {\n",
    "            'conversations':line['conversations']\n",
    "        }\n",
    "        json.dump(new_dict, file)\n",
    "        file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conveting swe-instruct to conversation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"../data/questions-5.jsonl\"\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"../data/bibblansvarar-llama-3-aisweden.jsonl\"\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to format {\"conversations\":[{\"from\":\"human\",\"value\":\"text\"},{\"from\":\"gpt\",\"value\":\"text\"}]}\n",
    "mapper = {\"instruction\":\"human\", \"generation\":\"gpt\"}\n",
    "\n",
    "for line in data:\n",
    "    new_line = {\"conversations\": []}\n",
    "    for key, value in line.items():\n",
    "        if key == \"model_name\":\n",
    "            continue\n",
    "        new_line['conversations'].append({\"from\":mapper[key], \"value\":value})\n",
    "\n",
    "    with open(\"../data/BibblanSvarar-Llama3-generated.jsonl\", 'a', encoding='utf-8') as file:\n",
    "        json.dump(new_line, file)\n",
    "        file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge swe-instruct-slimorcaformat with SlimOrca-sv-11k-v2.jsonl and shuffle\n",
    "\n",
    "import json\n",
    "\n",
    "path = \"../data/SlimOrca-sv-11k-v2.jsonl\"\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "\n",
    "path = \"../data/swe-instruct-slimorcaformat.jsonl\"\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data2 = []\n",
    "    for line in file:\n",
    "        data2.append(json.loads(line))\n",
    "\n",
    "\n",
    "# Merge the two datasets and upload it to huggingface\n",
    "data3 = data + data2\n",
    "\n",
    "\n",
    "# Shuffle the data\n",
    "import random\n",
    "\n",
    "random.shuffle(data3)\n",
    "\n",
    "# Upload to huggingface\n",
    "with open(\"../data/sv-instruct-v1.jsonl\", 'w', encoding='utf-8') as file:\n",
    "    for line in data3:\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files='../data/hopkok-v1.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub('skvarre/hopkok-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Save Dataset to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"AI-Sweden-Models/BiaSWE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../data/BiaSWE.jsonl\"\n",
    "\n",
    "dataset.to_json(save_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a jsonl file\n",
    "import json\n",
    "\n",
    "with open(save_path, 'w', encoding='utf-8') as file:\n",
    "    for line in dataset:\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split around .user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "save_path = \"../data/bibblansvarar-llama-3-aisweden-2.jsonl\"\n",
    "\n",
    "# Open the save path file\n",
    "with open(save_path, 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Split the \"generation\" entry around \".user\" and save it\n",
    "for item in data:\n",
    "    item['generation'] = item['generation'].split('.user')[0]\n",
    "\n",
    "# Save the modified data to a new file\n",
    "output_path = save_path.replace('.jsonl', '-split.jsonl')\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    for item in data:\n",
    "        json.dump(item, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "print(f\"Split data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "bad_path = \"../data/bibblansvarar-llama-3-aisweden.jsonl\"\n",
    "good_path = \"../data/bibblansvarar-llama-3-aisweden-2-split.jsonl\"\n",
    "\n",
    "# Read the contents of the bad_path file\n",
    "with open(bad_path, 'r', encoding='utf-8') as file:\n",
    "    bad_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Read the contents of the good_path file\n",
    "with open(good_path, 'r', encoding='utf-8') as file:\n",
    "    good_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Replace the lines in bad_data with corresponding lines from good_data\n",
    "for i, line in enumerate(bad_data):\n",
    "    if line['generation'].endswith(\"Fok\"):\n",
    "        bad_data[i] = good_data[i]\n",
    "\n",
    "# Write the updated bad_data back to the bad_path file\n",
    "with open(bad_path, 'w', encoding='utf-8') as file:\n",
    "    for line in bad_data:\n",
    "        json.dump(line, file)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def verify_json_lines(file_path):\n",
    "    \"\"\"\n",
    "    Verifies if a file is a valid JSON Lines file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: The path to the JSON Lines file.\n",
    "    \n",
    "    Returns:\n",
    "    - valid_lines: Number of valid JSON lines.\n",
    "    - invalid_lines: Number of invalid JSON lines.\n",
    "    \"\"\"\n",
    "    valid_lines = 0\n",
    "    invalid_lines = 0\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            try:\n",
    "                json.loads(line)\n",
    "                valid_lines += 1\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Invalid JSON on line {line_number}: {line.strip()}\")\n",
    "                invalid_lines += 1\n",
    "    \n",
    "    return valid_lines, invalid_lines\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"../data/bibblansvarar-llama-3-aisweden.jsonl\"\n",
    "valid_lines, invalid_lines = verify_json_lines(file_path)\n",
    "print(f\"Valid lines: {valid_lines}, Invalid lines: {invalid_lines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more bad-examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "path = \"../corrected-examples-en-sv.jsonl\"\n",
    "\n",
    "data = {\n",
    "        \"en\":\"\",\n",
    "        \"sv\":\"\"\n",
    "}\n",
    "\n",
    "data['en'] = \"\"\"\"\"\"\n",
    "\n",
    "data['sv'] = \"\"\"\"\"\"\n",
    "\n",
    "with open(path, 'a', encoding='utf-8') as file:\n",
    "        json.dump(data, file)\n",
    "        file.write(\"\\n\")\n",
    "        # flush\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "path = \"../data/bibblansvarar.jsonl\"\n",
    "\n",
    "data = {\n",
    "        \"conversations\":[\n",
    "                {\"from\":\"human\",\"value\":\"\"},\n",
    "                {\"from\":\"gpt\",\"value\":\"\"}\n",
    "        ]\n",
    "}\n",
    "\n",
    "data['conversations'][0]['value'] = \"\"\"\"\"\"\n",
    "\n",
    "data['conversations'][1]['value'] = \"\"\"\"\"\"\n",
    "\n",
    "with open(path, 'a', encoding='utf-8') as file:                \n",
    "        json.dump(data, file)\n",
    "        file.write(\"\\n\")\n",
    "        # flush\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = \"../bezzerwizzer.jsonl\"\n",
    "\n",
    "data = {\n",
    "        \"conversations\":[\n",
    "                {\"from\":\"human\",\"value\":\"\"},\n",
    "                {\"from\":\"gpt\",\"value\":\"\"}\n",
    "        ]\n",
    "}\n",
    "\n",
    "data['conversations'][0]['value'] = \"\"\"\"\"\"\n",
    "\n",
    "data['conversations'][1]['value'] = \"\"\"\"\"\"\n",
    "\n",
    "with open(path, 'a', encoding='utf-8') as file:                \n",
    "        json.dump(data, file)\n",
    "        file.write(\"\\n\")\n",
    "        # flush\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"../data/bibblansvarar-llama-3-aisweden.jsonl\"\n",
    "\n",
    "# Read the data from the file\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Define the substring to search for\n",
    "substring = \"\"\n",
    "\n",
    "# Modify the data where the instruction has the given substring\n",
    "for line in data:\n",
    "    if substring in line['instruction']:\n",
    "        print(\"Found it!\")\n",
    "        # Modify the instruction or any other desired field\n",
    "        line['generation'] = \"\"\"\"\"\"\n",
    "\n",
    "# Save the modified data back to the file\n",
    "with open(path, 'w', encoding='utf-8') as file:\n",
    "    for line in data:\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean annotation in BiaSWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"../data/BiaSWE.jsonl\"\n",
    "\n",
    "# Read the data from the file\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator 1': {'category': 'Anti-feminism and denial of discrimination (Opposing feminism, believing that feminism is not necessary or that women should not have the same rights as men do. Includes: The belief that men are discriminated against/that discrimination doesn’t exist)',\n",
       "  'comment': None,\n",
       "  'hate_speech': 'Yes',\n",
       "  'misogyny': 'Yes',\n",
       "  'rating': '7'},\n",
       " 'annotator 2': {'category': 'Stereotype (A widely held but fixed and oversimplified image or idea of a particular type of person or thing. Includes: Gender essentialism/Benevolent sexism/Intersectional, identity-based misogyny/Transmisogyny and homophobia )',\n",
       "  'comment': \"The author thinks men are better survivors than women, doesn't express it very explicitly though but it is obvious.\",\n",
       "  'hate_speech': 'Yes',\n",
       "  'misogyny': 'Yes',\n",
       "  'rating': '5'},\n",
       " 'annotator 3': {'category': 'Anti-feminism and denial of discrimination (Opposing feminism, believing that feminism is not necessary or that women should not have the same rights as men do. Includes: The belief that men are discriminated against/that discrimination doesn’t exist)',\n",
       "  'comment': None,\n",
       "  'hate_speech': 'Yes',\n",
       "  'misogyny': 'Yes',\n",
       "  'rating': '6'},\n",
       " 'annotator 4': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[250]['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotation = {\n",
    "    \"hate_speech\": False,\n",
    "    \"misogyny\": False,\n",
    "}\n",
    "\n",
    "for line in data:\n",
    "    new_annotation = {\n",
    "        \"hate_speech\": False,\n",
    "        \"misogyny\": False,\n",
    "    }\n",
    "    hate_speech = 0\n",
    "    no_hate = 0\n",
    "    misogyny = 0\n",
    "    no_mis = 0\n",
    "    for annotation in line['annotations']:\n",
    "        annot_json = line['annotations'][annotation]\n",
    "        if annot_json != None:\n",
    "            if annot_json['hate_speech'] == \"Yes\":\n",
    "                hate_speech += 1\n",
    "            else:\n",
    "                no_hate += 1\n",
    "            if annot_json['misogyny'] == \"Yes\":\n",
    "                misogyny += 1\n",
    "            else:\n",
    "                no_mis += 1\n",
    "    if hate_speech >= no_hate:\n",
    "        new_annotation['hate_speech'] = True\n",
    "    if misogyny >= no_mis:\n",
    "        new_annotation['misogyny'] = True\n",
    "    line['annotations'] = new_annotation    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Ni som bor i hyreslägenhet! Varför i helvete gör ni det? Inte råd?: Hej!  Tycker de är allt för mycket folk som söker bostad och gnäller att det inte finns något.. Köp en för helvete! Vad gör ni av era pengar egentligen? Så min fråga är varför köper inte fler personer lägenhet? Varför super ni upp hela lönen istället för att spara till kontantinsats? Eller trivs ni så bra i hyresghetton?',\n",
       " 'annotations': {'hate_speech': True, 'misogyny': False}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified data back to the file\n",
    "new_path = \"../data/BiaSWE-annotated.jsonl\"\n",
    "with open(new_path, 'w', encoding='utf-8') as file:\n",
    "    for line in data:\n",
    "        json.dump(line, file)\n",
    "        file.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
