{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../scandeval_benchmark_results.jsonl\"\n",
    "with open(file, 'r') as f:\n",
    "    json_list = list(f)\n",
    "\n",
    "data = [json.loads(item) for item in json_list]\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mean_se(mean, se):\n",
    "    return f\"{mean:.2f} ± {se:.2f}\"\n",
    "\n",
    "results = []\n",
    "for _, row in df.iterrows():\n",
    "    task = row['task']\n",
    "    model = row['model']\n",
    "    dataset_languages = ', '.join(row['dataset_languages'])\n",
    "    results_dict = row['results']['total']\n",
    "\n",
    "    formatted_metrics = {}\n",
    "    for metric, value in results_dict.items():\n",
    "        if \"_se\" in metric:\n",
    "            continue\n",
    "        se_metric = metric + \"_se\"\n",
    "        formatted_metrics[metric] = format_mean_se(value, results_dict[se_metric])\n",
    "\n",
    "    # Model Information\n",
    "    num_params = f\"{int(row['num_model_parameters']):,}\"\n",
    "    max_seq_length = row['max_sequence_length']\n",
    "    vocabulary_size = row['vocabulary_size']\n",
    "    generative = row['generative']\n",
    "    few_shot = row['few_shot']\n",
    "    validation_split = row['validation_split']\n",
    "    scandeval_version = row['scandeval_version']\n",
    "    dataset = row['dataset']\n",
    "\n",
    "    results.append({\n",
    "        'Dataset': dataset,\n",
    "        'Task': task,\n",
    "        'Language(s)': dataset_languages,\n",
    "        'Model': model,\n",
    "        'Results': formatted_metrics,\n",
    "        'Model Information': {\n",
    "            'Number of Parameters': num_params,\n",
    "            'Max Sequence Length': max_seq_length,\n",
    "            'Vocabulary Size': vocabulary_size,\n",
    "            'Generative': generative,\n",
    "            'Few-shot': few_shot,\n",
    "            'Validation Split': validation_split,\n",
    "            'Scandeval Version': scandeval_version\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  merged-models/gpt-sw3-6.7b-hopkok-v3-nosystem-DPO\n",
      "---------------------------------------------\n",
      "swerec {'test_mcc': '67.31 ± 3.03', 'test_macro_f1': '66.82 ± 2.68'}\n",
      "suc3 {'test_micro_f1_no_misc': '31.97 ± 2.48', 'test_micro_f1': '20.12 ± 2.76'}\n",
      "scala-sv {'test_mcc': '7.46 ± 2.47', 'test_macro_f1': '50.99 ± 2.71'}\n",
      "scandiqa-sv {'test_em': '48.90 ± 1.25', 'test_f1': '55.62 ± 1.01'}\n",
      "swedn {'test_bertscore': '61.62 ± 2.50', 'test_rouge_l': '17.60 ± 0.90'}\n",
      "mmlu-sv {'test_mcc': '8.95 ± 1.51', 'test_accuracy': '31.67 ± 1.10'}\n",
      "hellaswag-sv {'test_accuracy': '33.32 ± 1.31', 'test_mcc': '11.63 ± 1.87'}\n",
      "speed {'test_speed': '5376.64 ± 1129.38', 'test_speed_short': '1300.07 ± 415.72'}\n"
     ]
    }
   ],
   "source": [
    "x = 11 # Change this to see different models\n",
    "\n",
    "print(\"Model: \", results[x*8]['Model'])\n",
    "print(\"---------------------------------------------\")\n",
    "for i in range(x*8, x*8+8):\n",
    "    print(results[i]['Dataset'], results[i]['Results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metric(metric_str):\n",
    "    \"\"\"Extract mean and standard deviation from metric string.\"\"\"\n",
    "    mean, std = metric_str.split(' ± ')\n",
    "    return float(mean), float(std)\n",
    "\n",
    "def std_deviation_away(your_metric, best_metric):\n",
    "    \"\"\"Calculate how many standard deviations your metric is away from the best metric.\"\"\"\n",
    "    your_mean, _ = parse_metric(your_metric)\n",
    "    best_mean, best_std = parse_metric(best_metric)\n",
    "    return abs(your_mean - best_mean) / best_std\n",
    "\n",
    "def calculate_rank(your_metrics, best_metrics):\n",
    "    \"\"\"Calculate the rank of your model based on ScandEval formula.\"\"\"\n",
    "    std_away_values = []\n",
    "\n",
    "    for category, metrics in your_metrics.items():\n",
    "        for metric_name, your_value in metrics.items():\n",
    "            best_value = best_metrics[category][metric_name]\n",
    "            std_away = std_deviation_away(your_value, best_value)\n",
    "            std_away_values.append(std_away)\n",
    "\n",
    "    avg_std_away = sum(std_away_values) / len(std_away_values)\n",
    "    return 1 + avg_std_away\n",
    "\n",
    "metrics_json = {}\n",
    "for i in range(8, 16):\n",
    "    metrics_json[results[i]['Dataset']] = results[i]['Results']\n",
    "\n",
    "metrics_json \n",
    "\n",
    "#Make a copy of metrics_json\n",
    "best_json = {k: v.copy() for k, v in metrics_json.items()}\n",
    "\n",
    "\n",
    "best_json['swerec']['test_mcc'] = '56.60 ± 3.37'\n",
    "best_json['swerec']['test_macro_f1'] = '62.73 ± 3.61'\n",
    "best_json['suc3']['test_micro_f1_no_misc'] = '14.58 ± 1.30'\n",
    "best_json['suc3']['test_micro_f1'] = '14.79 ± 1.27'\n",
    "best_json['scala-sv']['test_mcc'] = '10.92 ± 1.83'\n",
    "best_json['scala-sv']['test_macro_f1'] = '52.63 ± 2.98'\n",
    "best_json['scandiqa-sv']['test_em'] = '50.18 ± 0.54'\n",
    "best_json['scandiqa-sv']['test_f1'] = '57.90 ± 0.53'\n",
    "best_json['swedn']['test_bertscore'] = '64.89 ± 0.15'\n",
    "best_json['swedn']['test_rouge_l'] = '18.79 ± 0.22'\n",
    "best_json['mmlu-sv']['test_mcc'] = '6.16 ± 0.81'\n",
    "best_json['mmlu-sv']['test_accuracy'] = '28.35 ± 0.97'\n",
    "best_json['hellaswag-sv']['test_accuracy'] = '10.90 ± 0.86'\n",
    "best_json['hellaswag-sv']['test_mcc'] = '32.01 ± 0.54'\n",
    "best_json['speed']['test_speed'] = '2383 ± 451'\n",
    "best_json['speed']['test_speed_short'] = '718 ± 221'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_json, best_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_rank(metrics_json, best_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ScandEval from script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiaSWE Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#../data/BiaSWE-annotated-bias-gpt-sw3-6.7b-hopkok-v3-nosystem-DPO-Run-1.jsonl\n",
    "#../data/BiaSWE-annotated-bias-gpt-sw3-6.7b-v2-instruct.jsonl\n",
    "#../data/BiaSWE-annotated-bias-gpt-sw3-6.7b-hopkok-v3-nosystem.jsonl\n",
    "path = \"../data/BiaSWE-annotated-bias-gpt-sw3-6.7b-hopkok-v3-nosystem-DPO-Run-1.jsonl\"\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['annotations']['ground_truth']['hate_speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "y_true_hate_speech = []\n",
    "y_pred_hate_speech = []\n",
    "\n",
    "y_true_misogyny = []\n",
    "y_pred_misogyny = []\n",
    "\n",
    "for d in data:\n",
    "    annotations = d['annotations']\n",
    "    y_true_hate_speech.append(annotations['ground_truth']['hate_speech'])\n",
    "    y_pred_hate_speech.append(annotations['labels']['hate_speech'])\n",
    "\n",
    "    y_true_misogyny.append(annotations['ground_truth']['misogyny'])\n",
    "    y_pred_misogyny.append(annotations['labels']['misogyny'])\n",
    "\n",
    "\n",
    "f1_hate_speech = f1_score(y_true_hate_speech, y_pred_hate_speech)\n",
    "f1_misogyny = f1_score(y_true_misogyny, y_pred_misogyny)\n",
    "\n",
    "accuracy_hate_speech = accuracy_score(y_true_hate_speech, y_pred_hate_speech)\n",
    "accuracy_misogyny = accuracy_score(y_true_misogyny, y_pred_misogyny)\n",
    "\n",
    "y_true_combined = [(y_true_hate_speech[i], y_true_misogyny[i]) for i in range(len(y_true_hate_speech))]\n",
    "y_pred_combined = [(y_pred_hate_speech[i], y_pred_misogyny[i]) for i in range(len(y_pred_hate_speech))]\n",
    "\n",
    "joint_accuracy = sum([1 for i in range(len(y_true_combined)) if y_true_combined[i] == y_pred_combined[i]]) / len(y_true_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Hate Speech: 0.4275555555555556\n",
      "Accuracy for Misogyny: 0.5353333333333333\n",
      "Joint Accuracy: 0.41555555555555557\n",
      "F1-Score for Hate Speech: 0.016793893129770993\n",
      "F1-Score for Misogyny: 0.0760053026955369\n",
      "Macro-Averaged F1-Score: 0.046399597912653946\n"
     ]
    }
   ],
   "source": [
    "f1_hate_speech, f1_misogyny\n",
    "macro_f1 = (f1_hate_speech + f1_misogyny) / 2\n",
    "\n",
    "\n",
    "print(f\"Accuracy for Hate Speech: {accuracy_hate_speech}\")\n",
    "print(f\"Accuracy for Misogyny: {accuracy_misogyny}\")\n",
    "print(f\"Joint Accuracy: {joint_accuracy}\")\n",
    "print(f\"F1-Score for Hate Speech: {f1_hate_speech}\")\n",
    "print(f\"F1-Score for Misogyny: {f1_misogyny}\")\n",
    "print(f\"Macro-Averaged F1-Score: {macro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1_hate_speech: 0.6831262134969214\n",
      "Average f1_misogyny: 0.6259955918326965\n",
      "Average accuracy_hate_speech: 0.5748888888888889\n",
      "Average accuracy_misogyny: 0.5173333333333334\n",
      "Average joint_accuracy: 0.44333333333333336\n",
      "Average macro_f1: 0.6545609026648089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(data):\n",
    "    y_true_hate_speech = []\n",
    "    y_pred_hate_speech = []\n",
    "    y_true_misogyny = []\n",
    "    y_pred_misogyny = []\n",
    "\n",
    "    for d in data:\n",
    "        annotations = d['annotations']\n",
    "        y_true_hate_speech.append(annotations['ground_truth']['hate_speech'])\n",
    "        y_pred_hate_speech.append(annotations['labels']['hate_speech'])\n",
    "\n",
    "        y_true_misogyny.append(annotations['ground_truth']['misogyny'])\n",
    "        y_pred_misogyny.append(annotations['labels']['misogyny'])\n",
    "\n",
    "    f1_hate_speech = f1_score(y_true_hate_speech, y_pred_hate_speech)\n",
    "    f1_misogyny = f1_score(y_true_misogyny, y_pred_misogyny)\n",
    "\n",
    "    accuracy_hate_speech = accuracy_score(y_true_hate_speech, y_pred_hate_speech)\n",
    "    accuracy_misogyny = accuracy_score(y_true_misogyny, y_pred_misogyny)\n",
    "\n",
    "    y_true_combined = [(y_true_hate_speech[i], y_true_misogyny[i]) for i in range(len(y_true_hate_speech))]\n",
    "    y_pred_combined = [(y_pred_hate_speech[i], y_pred_misogyny[i]) for i in range(len(y_pred_hate_speech))]\n",
    "\n",
    "    joint_accuracy = sum([1 for i in range(len(y_true_combined)) if y_true_combined[i] == y_pred_combined[i]]) / len(y_true_combined)\n",
    "    macro_f1 = (f1_hate_speech + f1_misogyny) / 2\n",
    "\n",
    "    return {\n",
    "        'f1_hate_speech': f1_hate_speech,\n",
    "        'f1_misogyny': f1_misogyny,\n",
    "        'accuracy_hate_speech': accuracy_hate_speech,\n",
    "        'accuracy_misogyny': accuracy_misogyny,\n",
    "        'joint_accuracy': joint_accuracy,\n",
    "        'macro_f1': macro_f1\n",
    "    }\n",
    "\n",
    "num_runs = len(data) // 450\n",
    "run_chunks = [data[i*450:(i+1)*450] for i in range(num_runs)]\n",
    "\n",
    "# Store metrics for each run\n",
    "metrics_list = []\n",
    "\n",
    "for run_data in run_chunks:\n",
    "    metrics = calculate_metrics(run_data)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "# Compute average metrics\n",
    "average_metrics = {\n",
    "    'f1_hate_speech': np.mean([metrics['f1_hate_speech'] for metrics in metrics_list]),\n",
    "    'f1_misogyny': np.mean([metrics['f1_misogyny'] for metrics in metrics_list]),\n",
    "    'accuracy_hate_speech': np.mean([metrics['accuracy_hate_speech'] for metrics in metrics_list]),\n",
    "    'accuracy_misogyny': np.mean([metrics['accuracy_misogyny'] for metrics in metrics_list]),\n",
    "    'joint_accuracy': np.mean([metrics['joint_accuracy'] for metrics in metrics_list]),\n",
    "    'macro_f1': np.mean([metrics['macro_f1'] for metrics in metrics_list])\n",
    "}\n",
    "\n",
    "# Print average metrics\n",
    "for metric, average in average_metrics.items():\n",
    "    print(f\"Average {metric}: {average}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
