{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, get_peft_config, PeftModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/20_000_examples_train.pt'\n",
    "eval_path = 'data/20_000_examples_eval.pt'\n",
    "tokenizer = AutoTokenizer.from_pretrained('AI-Sweden-Models/gpt-sw3-6.7b')\n",
    "\n",
    "train_data = torch.load(train_path)\n",
    "eval_data = torch.load(eval_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load huggingface datset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Intel/orca_dpo_pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset as jsonl\n",
    "dataset['train'].to_json('../data/Orca-DPO-pairs.jsonl', orient='records', lines=True, mode='w')\n",
    "# Merge with validation\n",
    "# dataset['validation'].to_json('../data/oasst2_eval.jsonl', orient='records', lines=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"skvarre/SlimOrca-SV-33k\"\n",
    "\n",
    "# Upload dataset from json to huggingface\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create dataset from jsonl\n",
    "dataset = Dataset.from_json('../data/SlimOrca-sv-v2-33k.jsonl')\n",
    "\n",
    "dataset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"skvarre/hogskoleprovet-ord-3k\"\n",
    "\n",
    "# Download dataset from hub\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset['train'])\n",
    "\n",
    "for i in range(len(dataset['train'])):\n",
    "    dataset['train']['text'][i][1] += '.'\n",
    "\n",
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_example(example):\n",
    "    # Initialize empty strings for Human and GPT\n",
    "    human_text = \"\"\n",
    "    bot_text = \"\"\n",
    "\n",
    "    # Extract text entries\n",
    "    for text_entry in example['text']:\n",
    "        if text_entry['<human>'] is not None:\n",
    "            human_text = text_entry['<human>']\n",
    "        if text_entry['<bot>'] is not None:\n",
    "            bot_text = text_entry['<bot>']\n",
    "\n",
    "    # Return new format, ensuring that both parts are non-empty\n",
    "    if human_text and bot_text:\n",
    "        return {'human': human_text, 'gpt': bot_text}\n",
    "    else:\n",
    "        return {'human': None, 'gpt': None}  # Or handle it another way if preferred\n",
    "\n",
    "new_dataset = dataset.map(reformat_example, batched=False)\n",
    "new_dataset = new_dataset.filter(lambda x: x['human'] is not None and x['gpt'] is not None)\n",
    "# Delete text column\n",
    "new_dataset = new_dataset.remove_columns('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, StoppingCriteriaList, StoppingCriteria\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# (Optional) - define a stopping criteria\n",
    "# We ideally want the model to stop generate once the response from the Bot is generated\n",
    "class StopOnTokenCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return input_ids[0, -1] == self.stop_token_id\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"../models/gpt-sw3-6.7b-v2-translator-v2/checkpoint-30\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "stop_on_token_criteria = StopOnTokenCriteria(stop_token_id=pipe.tokenizer.bos_token_id)\n",
    "text = \"I like to eat ice cream in the summer.\"\n",
    "\n",
    "# This will translate English to Swedish\n",
    "# To translate from Swedish to English the prompt would be:\n",
    "# prompt = f\"<|endoftext|><s>User: Översätt till Engelska från Svenska\\n{text}<s>Bot:\"\n",
    "\n",
    "prompt = f\"<|endoftext|><s>User: Översätt till Svenska från Engelska\\n{text}<s>Bot:\"\n",
    "\n",
    "input_tokens = pipe.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "max_model_length = 2048\n",
    "dynamic_max_length = max_model_length - input_tokens.shape[1]\n",
    "\n",
    "response = pipe(\n",
    "    prompt,\n",
    "    max_length=dynamic_max_length,\n",
    "    truncation=True,\n",
    "    stopping_criteria=StoppingCriteriaList([stop_on_token_criteria])\n",
    ")\n",
    "\n",
    "print(response[0][\"generated_text\"].split(\"<s>Bot: \")[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I like to eat ice cream in the summer.\"\n",
    "\n",
    "# This will translate English to Swedish\n",
    "# To translate from Swedish to English the prompt would be:\n",
    "# prompt = f\"<|endoftext|><s>User: Översätt till Engelska från Svenska\\n{text}<s>Bot:\"\n",
    "\n",
    "prompt = f\"<|endoftext|><s>User: Översätt till Svenska från Engelska\\n{text}<s>Bot:\"\n",
    "\n",
    "input_tokens = pipe.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "max_model_length = 2048\n",
    "dynamic_max_length = max_model_length - input_tokens.shape[1]\n",
    "\n",
    "response = pipe(\n",
    "    prompt,\n",
    "    max_length=dynamic_max_length,\n",
    "    truncation=True,\n",
    "    stopping_criteria=StoppingCriteriaList([stop_on_token_criteria])\n",
    ")\n",
    "\n",
    "print(response[0][\"generated_text\"].split(\"<s>Bot: \")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, StoppingCriteriaList, StoppingCriteria, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# Save model to memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\"AI-Sweden-Models/gpt-sw3-6.7b-v2-instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AI-Sweden-Models/gpt-sw3-6.7b-v2-instruct\")\n",
    "model.save_pretrained(\"../models/gpt-sw3-6.7b-v2-instruct\")\n",
    "tokenizer.save_pretrained(\"../models/gpt-sw3-6.7b-v2-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model to hub\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/mnt/pr_SharedNLU/users/tim_olsen/instruction-tuning-nordic/models/qlora-test-gpt-sw3-6.7b-v2-20k-8batch-0.85split/checkpoint-3450\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/pr_SharedNLU/users/tim_olsen/instruction-tuning-nordic/models/qlora-test-gpt-sw3-6.7b-v2-20k-8batch-0.85split/checkpoint-3450\")\n",
    "\n",
    "model.push_to_hub(\"skvarre/qlora-4bit-test-gpt-sw3-6.7b\")\n",
    "tokenizer.push_to_hub(\"skvarre/qlora-4bit-test-gpt-sw3-6.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapters with base model and upload to hub\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import get_peft_model, get_peft_config, PeftModelForCausalLM, PeftModel \n",
    "import torch\n",
    "\n",
    "base_model_path = \"../merged-models/gpt-sw3-6.7b-hopkok-v3-nosystem\"\n",
    "adapter_path = \"../dpo_models/gpt-sw3-6.7b-hopkok-v3-nosystem-DPO-Run-1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     # Load model in 4-bit mode\n",
    "    bnb_4bit_use_double_quantization=True, # Nested quantization \n",
    "    bnb_4bit_quant_type=\"nf4\",             # Quantization algorithm to use \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # data type of model after quantization\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            # quantization_config=quantization_config,\n",
    "            local_files_only=False,\n",
    "            device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model_to_merge = PeftModel.from_pretrained(\n",
    "            base_model,\n",
    "            adapter_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model_to_merge.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_model = model_to_merge.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(\"../merged-models/gpt-sw3-6.7b-hopkok-v3-nosystem-DPO\")\n",
    "tokenizer.save_pretrained(\"../merged-models/gpt-sw3-6.7b-hopkok-v3-nosystem-DPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "merged_model = AutoModelForCausalLM.from_pretrained(\"../merged-models/gpt-sw3-6.7b-v2-hopkok-v1-instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../merged-models/gpt-sw3-6.7b-v2-hopkok-v1-instruct\")\n",
    "\n",
    "merged_model.push_to_hub(\"skvarre/gpt-sw3-6.7b-v2-hopkok-v1-instruct\")\n",
    "tokenizer.push_to_hub(\"skvarre/gpt-sw3-6.7b-v2-hopkok-v1-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, get_peft_config, PeftModelForCausalLM\n",
    "import torch\n",
    "# Save model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"skvarre/gpt-sw3-6.7b-v2-qlora-4bit-openhermes-28k-merged\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skvarre/gpt-sw3-6.7b-v2-qlora-4bit-openhermes-28k-merged\")\n",
    "\n",
    "model.save_pretrained(\"./gpt-sw3-6.7b-v2-qlora-4bit-openhermes-28k-merged\")\n",
    "tokenizer.save_pretrained(\"./gpt-sw3-6.7b-v2-qlora-4bit-openhermes-28k-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, SeamlessM4Tv2Model\n",
    "\n",
    "model = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "\n",
    "# Save to disk\n",
    "model.save_pretrained(\"../../models/seamless-m4t-v2-large\")\n",
    "processor.save_pretrained(\"../../models/seamless-m4t-v2-large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging adapters by dequantizing the model, instead of naively merging with base model. From [This Notebook](https://colab.research.google.com/drive/12c_sx8pIwiStqKr_7CF5BVwyyJpXmMTf?usp=sharing#scrollTo=c9yLWqKRKKyd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import peft\n",
    "import json\n",
    "import shutil\n",
    "from peft.utils import _get_submodules\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "model_name = \"../merged-models/gpt-sw3-6.7b-hopkok-v2-nosystem\"\n",
    "adapter = \"../dpo_models/gpt-sw3-6.7b-hopkok-v2-nosystem-DPO-TEST-3/\"\n",
    "\n",
    "def dequantize_model(model, to='./dequantized_model', dtype=torch.bfloat16, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    'model': the peftmodel you loaded with qlora.\n",
    "    'tokenizer': the model's corresponding hf's tokenizer.\n",
    "    'to': directory to save the dequantized model\n",
    "    'dtype': dtype that the model was trained using\n",
    "    'device': device to load the model to\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    os.makedirs(to, exist_ok=True)\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, cls):\n",
    "                print(f\"Dequantizing `{name}`...\")\n",
    "                quant_state = copy.deepcopy(module.weight.quant_state)\n",
    "                quant_state.dtype = dtype\n",
    "\n",
    "                weights = dequantize_4bit(module.weight.data, quant_state=quant_state, quant_type=\"nf4\").to(dtype)\n",
    "\n",
    "                new_module = torch.nn.Linear(module.in_features, module.out_features, bias=None, dtype=dtype)\n",
    "                new_module.weight = torch.nn.Parameter(weights)\n",
    "                new_module.to(device=device, dtype=dtype)\n",
    "\n",
    "                parent, target, target_name = _get_submodules(model, name)\n",
    "                setattr(parent, target_name, new_module)\n",
    "\n",
    "        # a hack, setting this to avoid hf's saving error because hf\n",
    "        # itself does not support saving a model that is registered to be loaded in 4bit.\n",
    "        model.is_loaded_in_4bit = False\n",
    "\n",
    "        print(\"Saving dequantized model...\")\n",
    "        model.save_pretrained(to)\n",
    "        #tokenizer.save_pretrained(to)\n",
    "        config_data = json.loads(open(os.path.join(to, 'config.json'), 'r').read())\n",
    "        config_data.pop(\"quantization_config\", None)\n",
    "        config_data.pop(\"pretraining_tp\", None)\n",
    "        with open(os.path.join(to, 'config.json'), 'w') as config:\n",
    "            config.write(json.dumps(config_data, indent=2))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"Starting to load the model {model_name} into memory\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map={\"\": 0}\n",
    "    )\n",
    "    print(model)\n",
    "    model = dequantize_model(model, to='./dqz_model/',dtype=dtype)\n",
    "    print(model)\n",
    "    model = PeftModel.from_pretrained(model, adapter)\n",
    "    print(model)\n",
    "    model = model.merge_and_unload()\n",
    "    print(model)\n",
    "\n",
    "    print(f\"Successfully loaded the model {model_name} into memory\")\n",
    "    model.save_pretrained(\"./models/\", safe_serialization=True)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Delete the model object if it exists\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "\n",
    "    # Clear the GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Run the garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Model, GPU cache, and garbage have been cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../merged-models/gpt-sw3-6.7b-hopkok-v2-nosystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"\"\"{{ eos_token }}{{ bos_token }}{{'\\n'}}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'USER:\\n' + message['content'] + '\\n'}}{{ bos_token }}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT:\\n' + message['content']}}{% endif %}{% endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"../merged-models/gpt-sw3-6.7b-hopkok-v2-nosystem\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
