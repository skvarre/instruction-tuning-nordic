{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edgelab/Tim/miniconda3/envs/pytorch-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "135780864"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"AI-Sweden-Models/gpt-sw3-126m\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Get number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jag är en AI och jag är en robot.\n",
      "\n",
      "Jag är en AI och jag är en robot.\n",
      "\n",
      "Jag är en AI och jag är en robot.\n",
      "\n",
      "Jag är en AI och jag är en robot.\n",
      "\n",
      "Jag är en AI och jag är en robot.\n",
      "\n",
      "Jag är en AI och jag är en robot.\n",
      "\n",
      "Jag är en AI och jag är en robot.\n",
      "\n",
      "Jag är en AI och jag är en robot.\n",
      "\n",
      "Jag är en AI och jag är en robot.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Jag är en AI och jag\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "generated_token_ids = model.generate(\n",
    "    inputs = input_ids,\n",
    "    max_new_tokens = 100,\n",
    "    do_sample=True,\n",
    "    temperature = 0.6,\n",
    "    top_p=1\n",
    ")[0]\n",
    "\n",
    "generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'<human>': 'Vad var den stora lögnen angående det amerikanska presidentvalet 2020?'}, {'<bot>': 'Den stora lögnen avser det falska påståendet från tidigare president Donald Trump att valet stals genom omfattande valfusk. Det finns dock inga bevis som stöder detta påstående och valresultatet har certifierats som korrekt av statliga och federala myndigheter.'}, {'<human>': 'Varför var tron så utbredd?'}]\n",
      "{'<bot>': 'För det första, år 2016 trodde Trump att han skulle förlora valet och började säga att Amerikas val var \"riggade\". Trump lyckades fortfarande vinna trots att han hade färre röster totalt. Men år 2020 fick Trump 10 miljoner fler röster totalt än år 2016, men han sades fortfarande förlora valet. \\nDessutom, natten innan valet slutade, var rösterna till förmån för Trump. Men poströstsedlarna, som vanligtvis räknas på natten, var inte till Trumps fördel. Morgonen efter det var Trump inte längre i ledningen. \\nPå en något annorlunda anmärkning är en annan möjlig orsak till detta faktiskt internet. Sociala medieföretag personifierar innehållet som deras användare ser baserat på deras tidigare beteende, för att vara mer attraktiva för användare. Men detta skulle få Trump-supportrar att se mer och mer information som hävdar att denna tro är sann. Att se en överväldigande majoritet av deras information stödja denna tro kommer i sin tur att få användare att tro att det är mer sant. \\nDessa kan ha varit några orsaker till att denna tro att rösterna var \"riggade\" är så utbredd.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "path = \"../../final/smaller_test.jsonl\"\n",
    "\n",
    "with open(path, \"r\") as file:\n",
    "    data = [json.loads(line) for line in file] # jsonl file \n",
    "\n",
    "print(data[15]['text'][:-1])\n",
    "print(data[15]['text'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|endoftext|>\n",
      "<s>User\n",
      "Jan Frederik Veldkamp (31 March 1941, Amsterdam - 12 November 2017) was a Dutch botanist. The standard author abbreviation Veldkamp is used to indicate this person as the author when citing a botanical name.\n",
      "<s>User\n",
      "What is known about the author Jan Frederik Veldkamp?\n",
      "<s>Bot\n",
      "Jan Frederik Veldkamp Jan Frederik Veldkamp (31 March 1941, Amsterdam - 12 November 2017) was a Dutch Botanist.\n",
      "<s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[63423,    18,     3, 63423,    18,     2, 15088,    18, 30777, 15081,\n",
       "           435,   825,  9980,   383, 63480, 63456,  7464, 63423, 63456, 63491,\n",
       "         63489, 63456, 63446, 19171,   381, 63423, 63456, 63459,  7540, 63423,\n",
       "         63459, 63455, 63456, 63499, 63462,   545,   268, 23730, 39040,   412,\n",
       "         63443,   619,  3844,  4921, 62499, 13841,   435,   825,  9980,   428,\n",
       "          2067,   341, 19440,   593,   854,   578,   306,  4921,  1206,  5888,\n",
       "           291,   268, 39040,  1111,  1998, 63443,    18,     2, 15088,    18,\n",
       "          4950,   428,  4191,   998,   306,  4921,  5526, 15081,   435,   825,\n",
       "          9980, 63495,    18,     2, 22493,    18, 30777, 15081,   435,   825,\n",
       "          9980,  5526, 15081,   435,   825,  9980,   383, 63480, 63456,  7464,\n",
       "         63423, 63456, 63491, 63489, 63456, 63446, 19171,   381, 63423, 63456,\n",
       "         63459,  7540, 63423, 63459, 63455, 63456, 63499, 63462,   545,   268,\n",
       "         23730, 51498,   412, 63443,    18,     2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datahandler import tokenize \n",
    "\n",
    "path = \"100_examples.jsonl\"\n",
    "\n",
    "with open(path, \"r\") as file: \n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "tokenize(data[24], '<s>', '<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tok import tokenize_file, CHAT_TURN_FORMATS, ROLEMAP \n",
    "\n",
    "SPECIAL_TOKENS = tokenizer.special_tokens_map\n",
    "EOS_TOKEN = SPECIAL_TOKENS[\"eos_token\"]\n",
    "BOS_TOKEN = SPECIAL_TOKENS[\"bos_token\"]\n",
    "\n",
    "SPECIAL_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4237/103996050.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2, 63423, 45416, 63457, 63523, 32650, 63503, 12273,  1423, 29190,\n",
       "         15167,   268, 13555, 10495,   346,   268,  4399,   564, 10621,  6461,\n",
       "           268, 11877,   515,   268, 41010,   606, 12111,   623, 63446,   600,\n",
       "          2587, 45884, 63446,   348,  4328,   504, 41549, 39652, 63423,     3,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data(samples : list):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for sample in samples: \n",
    "        context = sample['text'][:-1]\n",
    "        response = sample['text'][-1]\n",
    "        inputs.append(f\"{BOS_TOKEN} {context} {EOS_TOKEN}\")\n",
    "        targets.append(f\"{response} {EOS_TOKEN}\")\n",
    "    return inputs, targets\n",
    "\n",
    "# Example usage\n",
    "inputs, targets = prepare_data(data)\n",
    "\n",
    "max_length = 48\n",
    "input_encodings = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length, add_special_tokens=True)\n",
    "target_encodings = tokenizer(targets, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length, add_special_tokens=True)\n",
    "\n",
    "# Make sure that we use Dataset class\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "train_dataset = TextDataset(input_encodings, target_encodings.input_ids)\n",
    "train_dataset.encodings[\"input_ids\"]\n",
    "len(train_dataset[0][\"input_ids\"])\n",
    "\n",
    "split = 0.8\n",
    "n = len(train_dataset)\n",
    "train_size = int(split * n)\n",
    "val_size = n - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "train_dataset[0]\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Dataloader \n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    # max_steps=500,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=max_length\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vad är 4 plus 4? 3.25*162.5\\xa0m\\xa0Vatten\\xa0och\\xa0vätska\\xa0och\\xa0elektrolyter\\xa0(i)'}, {'<human>': 'Finns det några specifika säkerhetsriktlinjer att följa när man använder Pregabalin Krka?'}, {'<bot>': 'Pregabalin Krka, Följande försiktighetsåtgärder gäller: Spårbarhet För att underlätta spårbarhet av biologiska läkemedel ska läkemedlets namn och tillverkningssatsnummer dokumenteras., Pediatrisk population Användning av pregabalin till barn rekommenderas inte på grund av risken för överdosering. Pediatrisk population ska inte övervakas med avseende på säkerhet eller effekt hos barn under 18 år., Administrering av pregabalin till barn ska ske under medicinsk övervakning av läkare med erfarenhet av behandling av pediatrisk population., Interaktioner och kontraindikationer Interaktioner med andra mediciner har\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Create a prompt\n",
    "prompt = \"<s>Vad är 4 plus 4?<|endoftext|>\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "# Generate a response\n",
    "generated_token_ids = model.generate(\n",
    "    inputs = input_ids,\n",
    "    max_new_tokens = 200,\n",
    "    do_sample=True,\n",
    "    temperature = 0.6,\n",
    "    top_p=1\n",
    ")[0]\n",
    "\n",
    "generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
